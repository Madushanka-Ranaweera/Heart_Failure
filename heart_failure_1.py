# -*- coding: utf-8 -*-
"""Heart_Failure.1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GiHHkgx8N0w5UCMsPoADAigM--3Vwmfc
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import seaborn as sns
import copy
import warnings
warnings.simplefilter(action="ignore", category=FutureWarning)

dataset = pd.read_csv(r"/content/heart.csv")
df = pd.DataFrame(dataset)
df

"""# **Preprocessing**"""

print("Dataframe information:\n")
display(df.info())

#Checking the missing value
df.isnull().sum()

#Visualizing the missing value by seaborn library
sns.heatmap(df.isnull(),cmap = 'magma',cbar = False);

print(f"Count of rows and columns in this dataset : {df.shape}\n")

df_cat = df.select_dtypes(include='object')
df_num = df.select_dtypes(exclude='object')
df_cat_dum = pd.get_dummies(df_cat)
df_num = df_num.drop('HeartDisease',axis=1)
df_cat_dum

df_final = pd.concat([df_num, df_cat_dum],axis=1)
df_final

df_final.nunique()

fig , ax = plt.subplots(figsize=(13,6))   
sns.heatmap(df.corr(), cmap="coolwarm", linecolor='white' , annot=True , linewidths=1 , ax=ax )



"""### **Data Train and Test**"""

x = df_final
y = df["HeartDisease"]

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=21)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train) 
x_test = scaler.fit_transform(x_test)

x_train = pd.DataFrame(x_train, columns=x.columns)
x_test = pd.DataFrame(x_test, columns=x.columns)

print("x train :\n")
display(x_train.head())
print("x test :\n")
display(x_test.head())

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score

from sklearn.linear_model import LogisticRegression

df_logreg = pd.DataFrame()
penalty_param = ['l1', 'l2']
states_num = range(32,43)

for p in penalty_param :
    for i in range(20,31) :
        for  s in states_num :
            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=i, random_state=s)
            logreg = LogisticRegression(solver='liblinear', max_iter=1000)
            logreg.fit(x_train, y_train)
            y_pred_logreg = logreg.predict(x_test)
            dict1 = {"Test Size": i, "Penalty":p, "Random State":s,
                     "Score" : logreg.score(x,y) , "Accuracy_score" : metrics.accuracy_score(y_test , y_pred_logreg )}
            df_logreg = df_logreg.append(dict1, ignore_index=True)

df_logreg.round(2)

indx_names = df_logreg[df_logreg['Accuracy_score'] < 0.90].index
df_logreg.drop(indx_names, inplace = True)
df_logreg = df_logreg.sort_values(by='Accuracy_score', ascending=False)
df_logreg

#Selecting above value and putting those for get some new results
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=34)
classifier_1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)
classifier_1.fit(x_train, y_train)
y_pred_1 = classifier_1.predict(x_test)

log_train = round(classifier_1.score(x_train, y_train) * 100, 2)
log_accuracy = round(accuracy_score(y_pred_1, y_test) * 100, 2)
log_f1 = round(f1_score(y_pred_1, y_test) * 100, 2)

print("Training Accuracy    :",log_train,"%")
print("Model Accuracy Score :",log_accuracy,"%\n")
print("--------------------------------------------------------\n")
print("Classification_Report: \n",classification_report(y_test,y_pred_1))

c_matrix = confusion_matrix(y,classifier_1.predict(x))

fig , ax = plt.subplots(figsize=(6,6))
sns.heatmap(c_matrix , annot=True , ax=ax , fmt='d')
ax.set_xlabel("predicted" , fontsize=20)
ax.xaxis.set_label_position('top')
ax.xaxis.set_ticklabels(['predicted 0s', 'predicted 1s'], fontsize = 10)
ax.xaxis.tick_top()

ax.set_ylabel('True', fontsize=20)
ax.yaxis.set_ticklabels(['Actual 0s', 'Actual 1s'], fontsize = 10)

plt.show()

print('Confusion matrix:\n\n', c_matrix)
print("-----------------------------")
print('\nTrue Positives(TP) = ', c_matrix[0,0])
print("-----------------------------")
print('\nTrue Negatives(TN) = ', c_matrix[1,1])
print("-----------------------------")
print('\nFalse Positives(FP) = ', c_matrix[0,1])
print("-----------------------------")
print('\nFalse Negatives(FN) = ', c_matrix[1,0])

"""### **Linear Regression**"""

from sklearn.linear_model import LogisticRegression 
logistic_model = LogisticRegression()
logistic_model.fit(x_train, y_train)

y_pred = logistic_model.predict(x_test)

accuracy = accuracy_score(y_test,y_pred)*100
print(accuracy)

"""### **K-NN**"""

from sklearn.neighbors import KNeighborsClassifier

x_train , x_test , y_train , y_test = train_test_split(x, y, test_size=0.2, random_state=32)
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
y_pred_knn = knn.predict(x_test)
accuracy = accuracy_score(y_test,y_pred_knn)*100
print(accuracy)

"""### **SVM**"""

from sklearn.svm import SVC

x_train , x_test , y_train , y_test = train_test_split(x, y, test_size=0.2, random_state=125)
SVM = SVC(kernel='rbf')
SVM.fit(x_train, y_train)
y_pred_svm = SVM.predict(x_test)
accuracy = accuracy_score(y_test,y_pred_svm)*100
print(accuracy)

"""### **Decision Tree**"""

from sklearn.tree import DecisionTreeClassifier

x_train , x_test , y_train , y_test = train_test_split(x, y, test_size=0.20, random_state=125)
DTs = DecisionTreeClassifier()
DTs.fit(x_train, y_train)
y_pred_DTs = DTs.predict(x_test)

print("Accuracy:",(metrics.accuracy_score(y_test, y_pred_DTs))*100)

"""### **Naive Bayes**"""

from sklearn.naive_bayes import GaussianNB

x_train , x_test , y_train , y_test = train_test_split(x, y, test_size=0.20, random_state=125)
gassianNB = GaussianNB()
gassianNB.fit(x_train, y_train)
y_pred_gassianNB = gassianNB.predict(x_test)

print("Accuracy:",(metrics.accuracy_score(y_test, y_pred_gassianNB))*100)

